'''Workflow for the CAMP short read assembly module.'''


from contextlib import redirect_stderr
import os
from os import makedirs
from os.path import basename, join
import pandas as pd
from utils import Workflow_Dirs, ingest_samples


# Load and/or make the working directory structure
dirs = Workflow_Dirs(config['work_dir'], "enzymetrics_protein_catalog")


# Load sample names and input files 
SAMPLES = ingest_samples(config['samples'], dirs.TMP)
    
# --- Workflow output --- #

rule all:
    input:
        join(dirs.OUT, 'samples.csv')

# --- Workflow steps --- #

rule remove_adapters:
    input:
        fwd = join(dirs.TMP,'{sample}_1.fastq.gz'),
        rev = join(dirs.TMP,'{sample}_2.fastq.gz'),
    output:
        outfwd = join(dirs.TMP,'{sample}.adapter_removed.1.fastq.gz'),
        outrev = join(dirs.TMP,'{sample}.adapter_removed.2.fastq.gz')
    log:
        join(dirs.LOG, 'adapter_removal', '{sample}.out'),
    threads: 8,
    params:
        basename=join(dirs.TMP,'{sample}'),
        adapterloc=config['adapters']
    shell:
        """
        bbduk.sh in={input.fwd}  -Xmx60G  in2={input.rev}  out={output.outfwd}  out2={output.outrev}  interleaved=f  stats={params.basename}_stats_bbduk  overwrite=true  qout=33  trd=t  hdist=1  k=27  ktrim="r"  mink=8  overwrite=true  trimq=10  qtrim='rl'  threads={threads}  minlength=51  maxns=-1  minbasefrequency=0.05  ecco=f  prealloc=t  ref={params.adapterloc} &> {log}
        """

rule filter_host_reads:
    input:
        fwd = join(dirs.TMP,'{sample}.adapter_removed.1.fastq.gz'),
        rev = join(dirs.TMP,'{sample}.adapter_removed.2.fastq.gz')
    output:
        join(dirs.TMP,'{sample}.host_filtered.adapter_removed.1.fastq.gz'),
        join(dirs.TMP,'{sample}.host_filtered.adapter_removed.2.fastq.gz')
    log:
        join(dirs.LOG, 'host_removal', '{sample}.out'),
    threads: 8,
    params:
        basename=join(dirs.TMP,'{sample}'),
        reference_database=config['host_removal_reference_database']
    shell:
        """
        bowtie2 -x {params.reference_database} -1 {input.fwd} -2 {input.rev} --un-conc-gz {params.basename}.host_filtered.adapter_removed.fastq.gz --very-sensitive --threads {threads} | samtools view -F 4 -b > {params.basename}_mapped_and_unmapped.bam
        rm {params.basename}_mapped_and_unmapped.bam
        mv {params.basename}.host_filtered.adapter_removed.fastq.1.gz {params.basename}.host_filtered.adapter_removed.1.fastq.gz
        mv {params.basename}.host_filtered.adapter_removed.fastq.2.gz {params.basename}.host_filtered.adapter_removed.2.fastq.gz
        """

rule run_metaspades:
    input:
        fwd = join(dirs.TMP,'{sample}.host_filtered.adapter_removed.1.fastq.gz'),
        rev = join(dirs.TMP,'{sample}.host_filtered.adapter_removed.2.fastq.gz')
    output:
        join(dirs.OUT,'assembled','{sample}','contigs.fasta'),
    log:
        join(dirs.LOG, 'metaSPAdes', '{sample}.out'),
    threads: 16,
    params:
        metaspades_maxmem=config['metaspades_maxmem'],
        outpath=join(dirs.OUT,'assembled','{sample}'),
    shell:
        """
        metaspades.py --only-assembler  -1 {input.fwd} -2 {input.rev} -o {params.outpath} -t {threads} -m {params.metaspades_maxmem} &> {log}
        # clean up assembly files
        #xargs rm -r {params.outpath}/K* {params.outpath}/misc {params.outpath}/tmp
        """    

rule call_orfs:
    input:
        contigs=join(dirs.OUT,'assembled','{sample}','contigs.fasta'),
    output:
        join(dirs.OUT,'prokka','{sample}','{sample}_prokka.gbk'),
        join(dirs.OUT,'prokka','{sample}','{sample}_prokka.gff'),
        join(dirs.OUT,'prokka','{sample}','{sample}_prokka.fna'),
        join(dirs.OUT,'prokka','{sample}','{sample}_prokka.faa'),
        join(dirs.OUT,'prokka','{sample}','{sample}_prokka.sqn'),
        join(dirs.OUT,'prokka','{sample}','{sample}_prokka.ffn'),
        join(dirs.OUT,'prokka','{sample}','{sample}_prokka.fsa'),
        join(dirs.OUT,'prokka','{sample}','{sample}_prokka.tbl'),
        join(dirs.OUT,'prokka','{sample}','{sample}_prokka.err'),
        join(dirs.OUT,'prokka','{sample}','{sample}_prokka.log'),
        join(dirs.OUT,'prokka','{sample}','{sample}_prokka.txt'),
        join(dirs.OUT,'prokka','{sample}','{sample}_prokka.tsv'),
    log:
        join(dirs.LOG, 'prokka', '{sample}.out'),
    threads: 2,
    params:
        outdir=join(dirs.OUT,'prokka','{sample}'),
        sam='{sample}'
    shell:
        """
        prokka --centre X --compliant --force --cpus {threads} --outdir {params.outdir} --prefix {params.sam}_prokka {input.contigs} &> {log}
        """

rule cluster_orfs:
    input:
        expand(join(dirs.OUT,'prokka','{sample}','{sample}_prokka.faa'),sample=SAMPLES),
    output:
        join(dirs.OUT,'mmseqs','clusters_30perc_rep_seq.fasta'),
        join(dirs.OUT,'mmseqs','clusters_30perc_cluster.tsv'),
    log:
        join(dirs.LOG,'mmseqs2','mmseqs.log'),
    threads: config['threads']
    params:
        outdir=join(dirs.OUT,'mmseqs'),
    shell:
        """
        cat {input} > all_seq_data.faa
        mmseqs easy-cluster all_seq_data.faa {params.outdir}/clusters_30perc tmp --min-seq-id 0.3 --threads 10 -c 0.9 --cov-mode 1 &> {log}
        rm -rf tmp
        """

rule annotate_orfs_by_alignment:
    input:
        tsvfiles = expand(join(dirs.OUT,'prokka','{sample}','{sample}_prokka.tsv'),sample=SAMPLES)
    output:
        join(dirs.OUT,'annotation','aligned_orfs.tsv'),
        join(dirs.OUT,'annotation','prokka_annotation_data.tsv'),
    log:
        join(dirs.LOG,'annotation','annotation.log'),
    threads: config['threads']
    params:
        infile=join(dirs.OUT,'mmseqs','clusters_30perc_rep_seq.fasta'),
        outfile=join(dirs.OUT,'annotation','aligned_orfs.tsv'),
        outdir=join(dirs.OUT,'annotation'),
        annotationdb = config['annotation_diamond_db']
    shell:
        """
        mkdir -p {params.outdir}
        cat {input.tsvfiles} > {params.outdir}/prokka_annotation_data.tsv
        diamond blastp -k 1 --db {params.annotationdb} --query {params.infile} --threads {threads} -o {params.outfile} &> {log}
        """
rule merge_annotation_data:
    input:
        alignmentanno = join(dirs.OUT,'annotation','aligned_orfs.tsv'),
        prokkaanno = join(dirs.OUT,'annotation','prokka_annotation_data.tsv'),
    output:
        join(dirs.OUT,'annotation','merged_annotation_data.tsv'),
    params:
        annotationmaploc=config['uniref_annotation_mapping'],
        clusterout=join(dirs.OUT,'mmseqs','clusters_30perc_cluster.tsv'),
        mergedfile=join(dirs.OUT,'annotation','merged_annotation_data.tsv'),
    run:
        prokka = {}
        with open(input.prokkaanno) as f:
            for line in f:
                line=line.rstrip().split('\t')
                if line[1]=='CDS':
                    gene=line[0]
                    rest = line[3:7]
                    prokka[gene]=rest
        uniref = {}
        with open(input.alignmentanno) as f:
            for line in f:
                line=line.rstrip().split('\t')
                uniref[line[0]]=[line[1]]
        uniref_annotation = {}
        with open(params.annotationmaploc) as f:
            for line in f:
                line=line.rstrip().split(' ')
                uniref_annotation[line[0]]=[' '.join(line[1:])]
        conorfs = []
        with open(params.clusterout) as f:
            for line in f:
                conorfs.append(line.rstrip().split('\t')[0])
        conorfs = list(set(conorfs))
        with open(params.mergedfile,'w') as w:
            w.write('\t'.join(['gene_id','gene','ec_number','COG','product','uniref90_annotation_by_alignment']) + '\n')
            for c in conorfs:
                temp=[]
                temp.append(c)
                temp.extend(prokka[c])
                try:
                    temp.extend(uniref_annotation[uniref[c][0]])
                except:
                    temp.extend('No uniref90 annotation')
                w.write('\t'.join(temp)+'\n')

rule make_config:
    input:
        join(dirs.OUT,'mmseqs','clusters_30perc_rep_seq.fasta'),
        join(dirs.OUT,'annotation','merged_annotation_data.tsv'),
    output:
        join(dirs.OUT, 'samples.csv')
    run:
        out = []
        for i in input:
            out.append(i)
            df = pd.DataFrame(out)
        df.to_csv(str(output), index = False, header = False)



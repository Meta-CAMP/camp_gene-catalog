'''Workflow for the CAMP short read assembly module.'''


from contextlib import redirect_stderr
import os
from os import makedirs
from os.path import basename, join
import pandas as pd
from utils import Workflow_Dirs, ingest_samples
from collections import Counter
import pandas as pd
from Bio import SeqIO
from functools import reduce

# Load and/or make the working directory structure
dirs = Workflow_Dirs(config['work_dir'], "enzymetrics_protein_catalog")


# Load sample names and input files 
SAMPLES = ingest_samples(config['samples'], dirs.TMP)
    
# --- Workflow output --- #

rule all:
    input:
        join(dirs.OUT, 'samples.csv')

# --- Workflow steps --- #

rule call_orfs:
    input:
        contigs=join(dirs.TMP,'{sample}.fasta'),
    output:
        join(dirs.OUT,'bakta','{sample}','{sample}.faa'),
        join(dirs.OUT,'bakta','{sample}','{sample}.hypotheticals.tsv'),
        join(dirs.OUT,'bakta','{sample}','{sample}.log'),
        join(dirs.OUT,'bakta','{sample}','{sample}.tsv'),
        join(dirs.OUT,'bakta','{sample}','{sample}.hypotheticals.faa'),
        join(dirs.OUT,'bakta','{sample}','{sample}.embl'),
        join(dirs.OUT,'bakta','{sample}','{sample}.fna'),
        join(dirs.OUT,'bakta','{sample}','{sample}.ffn'),
        join(dirs.OUT,'bakta','{sample}','{sample}.txt'),
        join(dirs.OUT,'bakta','{sample}','{sample}.gbff'),
    log:
        join(dirs.LOG, 'bakta', '{sample}.out'),
    threads: config['call_orfs_threads'],
    resources:
        mem_mb = config['call_orfs_mem_mb'],
    params:
        outdir=join(dirs.OUT,'bakta','{sample}'),
        sam='{sample}',
        baktadb=config['baktadb']
    shell:
        """
        bakta --db {params.baktadb} --threads {threads} --output {params.outdir} --prefix {params.sam} {input.contigs} &> {log}
        """

rule clean_annotations:
    input:
        expand(join(dirs.OUT,'bakta','{sample}','{sample}.tsv'),sample=SAMPLES),
    output:
        join(dirs.OUT,'annotations','orf_annotations.tsv'),
    threads: 1,
    params:
        outdir=join(dirs.OUT,'annotations')
    run:
        os.system('mkdir -p %s'%params.outdir)
        data = []
        for i in input:
            data.append(pd.read_csv(i,skiprows=2,sep='\t'))
        data = pd.concat(data)
        data.to_csv(params.outdir + '/orf_annotations.tsv')
        
rule cluster_orfs:
    input:
        expand(join(dirs.OUT,'bakta','{sample}','{sample}.faa'),sample=SAMPLES),
    output:
        join(dirs.OUT,'mmseqs','genecat_rep_seq.fasta'),
        join(dirs.OUT,'mmseqs','genecat_cluster.tsv'),
    log:
        join(dirs.LOG,'mmseqs2','mmseqs.log'),
    threads: config['cluster_orfs_threads'],
    resources:
        mem_mb = config['cluster_orfs_mem_mb'],
    params:
        outdir=join(dirs.OUT,'mmseqs'),
        pid=config['genecat_cluster_percent_identity'],
        coverage=config['coverage'],
        mmseqs_runtype = config['mmseqs_command'],
    shell:
        """
        cat {input} > all_seq_data.faa
        mmseqs {params.mmseqs_runtype} all_seq_data.faa {params.outdir}/genecat tmp --min-seq-id {params.pid} --threads {threads} -c {params.coverage} --cov-mode 1 &> {log}
        rm -rf tmp
        """

rule filter_gene_catalog:
    input:
        genecat=join(dirs.OUT,'mmseqs','genecat_rep_seq.fasta'),
        clusterfile=join(dirs.OUT,'mmseqs','genecat_cluster.tsv'),
    output:
        join(dirs.OUT,'mmseqs','genecat_rep_seq_filtered.fasta'),
    threads: config['filter_gene_catalog_threads'],
    resources:
        mem_mb = config['filter_gene_catalog_mem_mb'],
    params:
        outdir=join(dirs.OUT,'mmseqs'),
        filterprev=config['min_gene_prevalence'],
    run:
        clusters = []
        with open(input.clusterfile) as f:
            for line in f:
                clusters.append(line.rstrip().split('\t'))
        congenes = [x[0] for x in clusters]
        clustersize = pd.DataFrame.from_dict(Counter(congenes),orient='index')
        clustersize['sample'] = [x.split('_')[0] for x in clustersize.index]
        clustersize.to_csv(params.outdir+'/genecat_cluster_sizes.csv')
        tokeep = clustersize[clustersize[0]>=int(params.filterprev)].index
        seqs = SeqIO.to_dict(SeqIO.parse(str(input.genecat), "fasta"))
        with open(params.outdir+'/genecat_rep_seq_filtered.fasta','w') as w:
            for i in tokeep:
                w.write('>' + i + '\n')
                w.write(str(seqs[i].seq) + '\n')

    
rule index_gene_catalog:
    input:
        repseqs = join(dirs.OUT,'mmseqs','genecat_rep_seq_filtered.fasta'),
    output:
        join(dirs.OUT,'mmseqs','genecat_rep_seq_filtered.dmnd'),
    threads: config['index_gene_catalog_threads'],
    resources:
        mem_mb = config['index_gene_catalog_mem_mb'],
    params:
        dbfile = join(dirs.OUT,'mmseqs','genecat_rep_seq_filtered.dmnd'),
    shell:
        """
        diamond makedb --db {params.dbfile} --in {input.repseqs} 
        """

    
rule run_alignments:
    input:
        fwd = join(dirs.TMP, '{sample}_1.fastq'),
        rev = join(dirs.TMP, '{sample}_2.fastq'),
        idx = join(dirs.OUT,'mmseqs','genecat_rep_seq_filtered.dmnd'),
    output:
        join(dirs.TMP,'relative_abundances','{sample}.tsv'),
    log:
        join(dirs.LOG,'alignments','{sample}.log'),
    threads: config['run_alignments_threads'],
    resources:
        mem_mb = config['run_alignments_mem_mb'],
    params:
        sampletempname=join(dirs.TMP,'relative_abundances','{sample}_temp.fastq'),
        outfile = join(dirs.TMP,'relative_abundances','{sample}.tsv'),
        blocksize = config['diamond_blocksize']
    shell:
        """
        cat {input.fwd} {input.rev} > {params.sampletempname}
        diamond blastx --db {input.idx} --query {params.sampletempname} -b {params.blocksize} -p {threads} -o {params.sampletempname}_output &> {log}
        cut -f2 {params.sampletempname}_output | sort | uniq -cd | sed "s/^[ \t]*//"| awk "{{print \$2,\$1}}" > {params.outfile}
        rm {params.sampletempname} {params.sampletempname}_output
        """

rule compute_relative_abundances:
    input:
        expand(join(dirs.TMP,'relative_abundances','{sample}.tsv'),sample=SAMPLES)
    output:
        join(dirs.OUT,'merged_abundance_data','gene_catalog_filtered_relative_abundances.tsv'),
        join(dirs.OUT,'merged_abundance_data','gene_catalog_filtered_counts.tsv'),
    resources:
        mem_mb = config['compute_relative_abundances_mem_mb'],
    params:
        annotationfile = join(dirs.OUT,'annotations','orf_annotations.tsv'),
        outdir = join(dirs.OUT,'merged_abundance_data'),
    run:
        genelengths = pd.read_csv(params.annotationfile,sep=',',index_col=6).loc[:,['Start','Stop']]
        genelengths['gene_length'] = genelengths.Stop - genelengths.Start
        aligned = []
        for i in input:
            d = pd.read_csv(i,index_col=0,sep=' ',header=None)
            d.columns=[i.split('/')[-1].replace('.tsv','')]
            aligned.append(d)
        out = reduce(lambda x, y: pd.merge(x, y, left_index = True,right_index = True), aligned)
        out2=pd.merge([out,genelengths],left_index=True,right_index=True).drop(['Start','Stop'],axis=1)
        out2.to_csv(params.outdir + '/gene_catalog_filtered_counts.tsv',sep='\t')
        for ii in out2.columns[1:]:
            temp = out2.loc[:,ii]/out2.gene_length
            temp2 = temp/temp.sum()
            out2.loc[:,ii] = temp2
        out2.to_csv(params.outdir + '/gene_catalog_filtered_relative_abundances.tsv',sep='\t')

rule make_config:
    input:
        join(dirs.OUT,'mmseqs','genecat_rep_seq_filtered.fasta'),
        join(dirs.OUT,'annotations','orf_annotations.tsv'),
        join(dirs.OUT,'merged_abundance_data','gene_catalog_filtered_relative_abundances.tsv'),
        join(dirs.OUT,'merged_abundance_data','gene_catalog_filtered_counts.tsv'),
    output:
        join(dirs.OUT, 'samples.csv'),
    run:
        out = []
        for i in input:
            out.append(i)
            df = pd.DataFrame(out)
        df.to_csv(str(output), index = False, header = False)


